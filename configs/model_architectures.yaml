# Defines generic parameters for different model sizes.
# These keys align with standard Hugging Face config attributes.

# -- GPT-2 Style Architectures --
gpt2-10m:
  # GPT-2 uses these specific keys
  n_layer: 4
  n_head: 4
  n_embd: 256

gpt2-100m:
  n_layer: 6
  n_head: 6
  n_embd: 384

# -- Llama/Mistral Style Architectures --
# Example for a hypothetical small Llama-style model
llama-15m:
  # Most modern transformers use these keys
  hidden_size: 384
  intermediate_size: 1024
  num_hidden_layers: 4
  num_attention_heads: 4
  num_key_value_heads: 2 # For Grouped-Query Attention