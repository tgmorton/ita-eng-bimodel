l1_dataset_path: /Users/thomasmorton/ita-eng-bimodel/data/tokenized/25_25_it_eng/l1_train
l2_dataset_path: /Users/thomasmorton/ita-eng-bimodel/data/tokenized/25_25_it_eng/l2_train
output_dir: output/25_25_it_eng
tokenizer_path: /Users/thomasmorton/ita-eng-bimodel/tokenizer/25_25_it_eng
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 8
- 16
- 32
- 64
- 128
- 138
- 256
- 276
- 414
- 512
- 551
- 689
- 827
- 964
- 1102
- 1240
- 1377
- 1515
- 1653
- 1791
- 1928
- 2066
- 2204
- 2341
- 2479
- 2617
- 2754
- 2892
- 3030
- 3167
- 3305
- 3443
- 3581
- 3718
- 3856
- 3994
- 4131
- 4269
- 4407
- 4544
- 4682
- 4820
- 4957
- 5095
- 5233
- 5371
- 5508
- 5646
- 5784
- 5921
- 6059
- 6197
- 6334
- 6472
- 6610
- 6748
- 6749
- 6750
- 6752
- 6756
- 6764
- 6780
- 6812
- 6876
- 7004
- 7183
- 7260
- 7618
- 8053
- 8488
- 8923
- 9358
- 9793
- 10228
- 10663
- 11098
- 11533
- 11968
- 12403
- 12838
- 13273
- 13708
- 14143
- 14578
- 15013
- 15448
- 15883
- 16318
- 16753
- 17188
- 17623
- 18058
- 18493
- 18928
- 19363
- 19798
- 20233
- 20668
- 21103
- 21538
- 21973
- 22408
- 22843
- 23278
- 23713
- 24148
- 24583
- 25018
- 25453
- 25888
- 26323
- 26758
- 27193
- 27628
- 28063
