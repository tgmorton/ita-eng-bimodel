l1_dataset_path: data/tokenized/25_25_it_eng/l1_train
l2_dataset_path: data/tokenized/25_25_it_eng/l2_train
output_dir: output/25_25_it_eng
tokenizer_path: tokenizer/25_25_it_eng
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 8
- 16
- 20
- 32
- 40
- 60
- 64
- 80
- 100
- 120
- 128
- 140
- 160
- 180
- 200
- 220
- 240
- 256
- 259
- 279
- 299
- 319
- 339
- 359
- 379
- 399
- 419
- 439
- 459
- 479
- 498
- 518
- 538
- 558
- 578
- 598
- 618
- 638
- 658
- 678
- 698
- 718
- 737
- 757
- 777
- 797
- 817
- 837
- 857
- 877
- 897
- 917
- 937
- 957
- 977
- 978
- 979
- 981
- 985
- 993
- 997
- 1009
- 1017
- 1037
- 1041
- 1057
- 1077
- 1097
- 1105
- 1117
- 1137
- 1157
- 1177
- 1197
- 1217
- 1233
- 1236
- 1256
- 1276
- 1296
- 1316
- 1336
- 1356
- 1376
- 1396
- 1416
- 1436
- 1456
- 1475
- 1495
- 1515
- 1535
- 1555
- 1575
- 1595
- 1615
- 1635
- 1655
- 1675
- 1695
- 1714
- 1734
- 1754
- 1774
- 1794
- 1814
- 1834
- 1854
- 1874
- 1894
- 1914
- 1934
- 1954
