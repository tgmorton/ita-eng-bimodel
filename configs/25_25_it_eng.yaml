l1_dataset_path: data/tokenized/25_25_it_eng/l1_train
l2_dataset_path: data/tokenized/25_25_it_eng/l2_train
output_dir: output/bilingual_sweep/25_25_it_eng
tokenizer_path: tokenizer/25_25_it_eng
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 5
- 8
- 10
- 15
- 16
- 20
- 25
- 30
- 32
- 35
- 39
- 44
- 49
- 54
- 59
- 64
- 69
- 73
- 78
- 83
- 88
- 93
- 98
- 102
- 107
- 112
- 117
- 122
- 127
- 132
- 137
- 141
- 146
- 151
- 156
- 161
- 166
- 171
- 175
- 180
- 185
- 190
- 195
- 200
- 204
- 209
- 214
- 219
- 224
- 229
- 234
- 239
- 240
- 241
- 243
- 245
- 247
- 251
- 255
- 257
- 262
- 268
- 271
- 274
- 279
- 285
- 291
- 296
- 302
- 303
- 308
- 314
- 319
- 325
- 331
- 336
- 342
- 348
- 353
- 359
- 365
- 367
- 370
- 376
- 382
- 388
- 393
- 399
- 405
- 410
- 416
- 422
- 427
- 433
- 439
- 444
- 450
- 456
- 462
- 467
- 473
- 479
- 484
- 490
- 496
- 501
- 507
- 513
- 519
