# Base configuration for a 10M parameter model training run
output_dir: "models/100M"
train_dataset_path: "data/tokenized/100M/train"

# --- Model ---
tokenizer_path: "tokenizer/100M"
model_arch_type: "gpt2"
train_from_scratch: true
model_size_tag: "gpt2-100m"

# --- Training Hyperparameters ---
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 5e-4
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: "cosine"
num_warmup_steps: 100

# --- Logging & Saving ---
logging_steps: 5
save_steps: 3

# --- Hardware & Seed ---
use_amp: true
num_workers: 4
seed: 42

checkpoint_schedule:
- 1
- 2
- 4
- 8
- 16
- 32
- 64
- 128
- 192
- 256
- 320
- 384
- 448
- 512
- 598
- 683
- 768
- 853
- 938
- 1024
- 1126
- 1229
- 1332
- 1434
- 1536
- 1638
- 1740
- 1843
- 1946
- 2048
- 2102
- 2156
- 2210
- 2264
- 2318
- 2372
- 2479
- 2533
- 2587
- 2641
- 2695
- 2802
- 2856
- 2910
- 2964
- 3018
- 3072
- 3126
- 3191
- 3256
- 3320
- 3385
- 3450
- 3514
- 3579
- 3644
- 3709
- 3774
- 3838
- 3903
- 3968
- 4032
- 4097
- 4162
- 4227
- 4292
- 4356
- 4421
- 4486
- 4550
- 4615
- 4680
- 4745
- 4810
- 4874
- 4939
- 5004
- 5068
- 5133
- 5198
- 5263
- 5328
- 5392
- 5457
- 5522
- 5586
- 5651
- 5716
- 5781
- 5846
- 5910
- 5975
- 6040
- 6104
- 6169
- 6234
- 6299
- 6364
- 6428
- 6493
- 6622
- 6687
- 6752
- 6878
- 7005
- 7132
- 7258
- 7384
- 7511
- 7638
- 7764
- 7891
- 8018
- 8144
- 8271
- 8398
- 8524
- 8650
- 8777
- 8904
- 9030
- 9156
- 9283
- 9410
- 9536
- 9663
- 9790
- 9916
- 10043
- 10170
- 10296
- 10422
- 10549
- 10676
- 10802
- 10928
- 11055
- 11182
- 11308
- 11434
- 11561
- 11688
- 11814
- 11941
- 12068
- 12194
- 12321
- 12448
- 12574
- 12700
- 12827
- 13076
- 13326
- 13575
- 13824
- 14074
- 14323
- 14573
- 14822
- 15071
- 15321
- 15570
- 15819
- 16069
- 16318
- 16568
- 16817
- 17066
- 17316
- 17565
- 17814
- 18064
- 18313
- 18562
- 18812
- 19061
- 19311
- 19560
- 19809
- 20059
- 20308
- 20557
- 20807
- 21056
- 21305
- 21555
- 21804
- 22054
- 22303
- 22552
- 22802
- 23051
- 23300
- 23550
- 23799
- 24049
- 24298
- 24547
- 24797
- 25046