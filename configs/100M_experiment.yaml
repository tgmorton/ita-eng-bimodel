# Base configuration for a 10M parameter model training run
output_dir: "models/100M"
train_dataset_path: "data/tokenized/100M/train"

# --- Model ---
tokenizer_path: "tokenizer/100M"
model_arch_type: "gpt2"
train_from_scratch: true
model_size_tag: "gpt2-100m"

# --- Training Hyperparameters ---
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 5e-4
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: "cosine"
num_warmup_steps: 100

# --- Logging & Saving ---
logging_steps: 5
save_steps: 3

# --- Hardware & Seed ---
use_amp: true
num_workers: 4
seed: 42

checkpoint_schedule:
- 1
- 2
- 4
- 8
- 16
- 32
- 64
- 128
- 192
- 256
- 320
- 384
- 448
- 512
- 598
- 683
- 768
- 853
- 938
- 1024
- 1126
- 1229
- 1332
- 1434
- 1536
- 1638
- 1740
- 1843
- 1946
- 2048
- 2102
- 2156
- 2210
- 2263
- 2316
- 2370
- 2424
- 2477
- 2530
- 2584
- 2638
- 2692
- 2746
- 2799
- 2906
- 2960
- 3014
- 3121
- 3186
- 3250
- 3315
- 3380
- 3445
- 3510
- 3574
- 3639
- 3704
- 3768
- 3833
- 3898
- 3963
- 4028
- 4092
- 4157
- 4222
- 4286
- 4351
- 4416
- 4481
- 4546
- 4610
- 4675
- 4740
- 4804
- 4869
- 4934
- 4999
- 5064
- 5128
- 5193
- 5258
- 5322
- 5387
- 5452
- 5517
- 5582
- 5646
- 5711
- 5776
- 5840
- 5905
- 5970
- 6035
- 6100
- 6164
- 6229
- 6294
- 6358
- 6423
- 6488
- 6553
- 6618
- 6747
- 6874
- 7000
- 7126
- 7253
- 7380
- 7506
- 7633
- 7760
- 7886
- 8013
- 8140
- 8266
- 8392
- 8519
- 8646
- 8772
- 8898
- 9025
- 9152
- 9278
- 9404
- 9531
- 9658
- 9784
- 9911
- 10038
- 10164
- 10291
- 10418
- 10544
- 10670
- 10797
- 10924
- 11050
- 11176
- 11303
- 11430
- 11556
- 11683
- 11810
- 11936
- 12063
- 12190
- 12316
- 12442
- 12569
- 12696
- 12822
- 13071
- 13321
- 13570
- 13819
- 14069
- 14318
- 14568
- 14817
- 15066
- 15316
- 15565
- 15814
- 16064
- 16313
- 16563
- 16812
- 17061
- 17311
- 17560
- 17809
- 18059
- 18308
- 18557
- 18807
- 19056
- 19306
- 19555
- 19804
- 20054
- 20303
- 20552
- 20802
- 21051
- 21300
- 21550
- 21799
- 22049
- 22298
- 22547
- 22797
- 23046
- 23295
- 23545
- 23794
- 24044
- 24293
- 24542
- 24792
- 25041