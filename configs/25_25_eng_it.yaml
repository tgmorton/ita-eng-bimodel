l1_dataset_path: /Users/thomasmorton/ita-eng-bimodel/data/tokenized/25_25_eng_it/l1_train
l2_dataset_path: /Users/thomasmorton/ita-eng-bimodel/data/tokenized/25_25_eng_it/l2_train
output_dir: output/25_25_eng_it
tokenizer_path: /Users/thomasmorton/ita-eng-bimodel/tokenizer/25_25_eng_it
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 8
- 16
- 32
- 64
- 128
- 256
- 435
- 512
- 870
- 1305
- 1740
- 2175
- 2610
- 3045
- 3480
- 3915
- 4350
- 4785
- 5220
- 5655
- 6090
- 6525
- 6960
- 7395
- 7830
- 8265
- 8700
- 9135
- 9570
- 10005
- 10440
- 10875
- 11310
- 11745
- 12180
- 12615
- 13050
- 13485
- 13920
- 14355
- 14790
- 15225
- 15660
- 16095
- 16530
- 16965
- 17400
- 17835
- 18270
- 18705
- 19140
- 19575
- 20010
- 20445
- 20880
- 21315
- 21316
- 21317
- 21319
- 21323
- 21331
- 21347
- 21379
- 21443
- 21453
- 21571
- 21591
- 21729
- 21827
- 21866
- 22004
- 22142
- 22279
- 22417
- 22555
- 22692
- 22830
- 22968
- 23106
- 23243
- 23381
- 23519
- 23656
- 23794
- 23932
- 24069
- 24207
- 24345
- 24482
- 24620
- 24758
- 24896
- 25033
- 25171
- 25309
- 25446
- 25584
- 25722
- 25859
- 25997
- 26135
- 26272
- 26410
- 26548
- 26686
- 26823
- 26961
- 27099
- 27236
- 27374
- 27512
- 27649
- 27787
- 27925
- 28063
