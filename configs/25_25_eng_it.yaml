l1_dataset_path: data/tokenized/25_25_eng_it/l1_train
l2_dataset_path: data/tokenized/25_25_eng_it/l2_train
output_dir: output/bilingual_sweep/25_25_eng_it
tokenizer_path: tokenizer/25_25_eng_it
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 6
- 8
- 12
- 16
- 18
- 23
- 29
- 32
- 35
- 40
- 46
- 52
- 57
- 63
- 64
- 69
- 75
- 80
- 86
- 92
- 97
- 103
- 109
- 114
- 120
- 126
- 128
- 131
- 137
- 143
- 149
- 154
- 160
- 166
- 171
- 177
- 183
- 188
- 194
- 200
- 205
- 211
- 217
- 223
- 228
- 234
- 240
- 245
- 251
- 257
- 262
- 268
- 274
- 280
- 281
- 282
- 284
- 285
- 288
- 290
- 295
- 296
- 300
- 305
- 310
- 312
- 315
- 319
- 324
- 329
- 334
- 339
- 344
- 349
- 353
- 358
- 363
- 368
- 373
- 378
- 382
- 387
- 392
- 397
- 402
- 407
- 412
- 417
- 421
- 426
- 431
- 436
- 441
- 446
- 451
- 455
- 460
- 465
- 470
- 475
- 480
- 484
- 489
- 494
- 499
- 504
- 509
- 514
- 519
