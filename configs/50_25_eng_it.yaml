l1_dataset_path: /Users/thomasmorton/ita-eng-bimodel/data/tokenized/50_25_eng_it/l1_train
l2_dataset_path: /Users/thomasmorton/ita-eng-bimodel/data/tokenized/50_25_eng_it/l2_train
output_dir: output/50_25_eng_it
tokenizer_path: /Users/thomasmorton/ita-eng-bimodel/tokenizer/50_25_eng_it
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 8
- 16
- 32
- 64
- 128
- 256
- 433
- 512
- 866
- 1299
- 1732
- 2165
- 2598
- 3031
- 3463
- 3896
- 4329
- 4762
- 5195
- 5628
- 6061
- 6494
- 6926
- 7359
- 7792
- 8225
- 8658
- 9091
- 9524
- 9956
- 10389
- 10822
- 11255
- 11688
- 12121
- 12554
- 12987
- 13419
- 13852
- 14285
- 14718
- 15151
- 15584
- 16017
- 16450
- 16882
- 17315
- 17748
- 18181
- 18614
- 19047
- 19480
- 19912
- 20345
- 20778
- 21211
- 21644
- 22077
- 22510
- 22943
- 23375
- 23808
- 24241
- 24674
- 25107
- 25540
- 25973
- 26405
- 26838
- 27271
- 27704
- 28137
- 28570
- 29003
- 29436
- 29868
- 30301
- 30734
- 31167
- 31600
- 32033
- 32466
- 32899
- 33331
- 33764
- 34197
- 34630
- 35063
- 35496
- 35929
- 36361
- 36794
- 37227
- 37660
- 38093
- 38526
- 38959
- 39392
- 39824
- 40257
- 40690
- 41123
- 41556
- 41989
- 42422
- 42855
- 42856
- 42857
- 42859
- 42863
- 42871
- 42887
- 42919
- 42983
- 42993
- 43111
- 43131
- 43269
- 43367
- 43406
- 43544
- 43682
- 43819
- 43957
- 44095
- 44232
- 44370
- 44508
- 44646
- 44783
- 44921
- 45059
- 45196
- 45334
- 45472
- 45609
- 45747
- 45885
- 46022
- 46160
- 46298
- 46436
- 46573
- 46711
- 46849
- 46986
- 47124
- 47262
- 47399
- 47537
- 47675
- 47812
- 47950
- 48088
- 48226
- 48363
- 48501
- 48639
- 48776
- 48914
- 49052
- 49189
- 49327
- 49465
- 49603
