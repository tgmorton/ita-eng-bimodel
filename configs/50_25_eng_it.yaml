l1_dataset_path: data/tokenized/50_25_eng_it/l1_train
l2_dataset_path: data/tokenized/50_25_eng_it/l2_train
output_dir: output/bilingual_sweep/50_25_eng_it
tokenizer_path: tokenizer/50_25_eng_it
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 6
- 8
- 12
- 16
- 17
- 23
- 29
- 32
- 34
- 40
- 46
- 51
- 57
- 62
- 64
- 68
- 74
- 79
- 85
- 91
- 96
- 102
- 107
- 113
- 119
- 124
- 128
- 130
- 136
- 141
- 147
- 152
- 158
- 164
- 169
- 175
- 181
- 186
- 192
- 197
- 203
- 209
- 214
- 220
- 226
- 231
- 237
- 242
- 248
- 254
- 256
- 259
- 265
- 271
- 276
- 282
- 287
- 293
- 299
- 304
- 310
- 316
- 321
- 327
- 332
- 338
- 344
- 349
- 355
- 361
- 366
- 372
- 377
- 383
- 389
- 394
- 400
- 406
- 411
- 417
- 422
- 428
- 434
- 439
- 445
- 451
- 456
- 462
- 467
- 473
- 479
- 484
- 490
- 496
- 501
- 507
- 512
- 518
- 524
- 529
- 535
- 541
- 546
- 552
- 558
- 559
- 560
- 562
- 563
- 566
- 568
- 573
- 574
- 578
- 583
- 588
- 590
- 593
- 598
- 603
- 608
- 613
- 618
- 622
- 627
- 632
- 637
- 642
- 647
- 652
- 657
- 662
- 667
- 672
- 677
- 681
- 686
- 691
- 696
- 701
- 706
- 711
- 716
- 721
- 726
- 731
- 736
- 740
- 745
- 750
- 755
- 760
- 765
- 770
- 775
- 780
- 785
- 790
- 795
- 800
