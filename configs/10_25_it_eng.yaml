l1_dataset_path: data/tokenized/10_25_it_eng/l1_train
l2_dataset_path: data/tokenized/10_25_it_eng/l2_train
output_dir: output/10_25_it_eng
tokenizer_path: tokenizer/10_25_it_eng
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 8
- 16
- 21
- 32
- 42
- 62
- 64
- 83
- 103
- 124
- 128
- 144
- 165
- 185
- 206
- 226
- 247
- 267
- 288
- 308
- 329
- 349
- 370
- 391
- 392
- 393
- 395
- 399
- 407
- 411
- 423
- 431
- 451
- 455
- 471
- 491
- 511
- 519
- 531
- 551
- 571
- 591
- 611
- 631
- 647
- 650
- 670
- 690
- 710
- 730
- 750
- 770
- 790
- 810
- 830
- 850
- 870
- 889
- 909
- 929
- 949
- 969
- 989
- 1009
- 1029
- 1049
- 1069
- 1089
- 1109
- 1128
- 1148
- 1168
- 1188
- 1208
- 1228
- 1248
- 1268
- 1288
- 1308
- 1328
- 1348
- 1368
