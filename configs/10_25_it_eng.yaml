l1_dataset_path: data/tokenized/10_25_it_eng/l1_train
l2_dataset_path: data/tokenized/10_25_it_eng/l2_train
output_dir: output/bilingual_sweep/10_25_it_eng
tokenizer_path: tokenizer/10_25_it_eng
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 6
- 8
- 11
- 16
- 21
- 26
- 31
- 32
- 36
- 41
- 46
- 51
- 56
- 61
- 66
- 71
- 76
- 81
- 86
- 91
- 97
- 98
- 99
- 101
- 103
- 105
- 109
- 113
- 114
- 120
- 126
- 129
- 131
- 137
- 143
- 148
- 154
- 160
- 161
- 165
- 171
- 177
- 182
- 188
- 194
- 199
- 205
- 211
- 216
- 222
- 225
- 228
- 233
- 239
- 244
- 250
- 256
- 261
- 267
- 273
- 278
- 284
- 290
- 295
- 301
- 307
- 312
- 318
- 324
- 329
- 335
- 341
- 346
- 352
- 358
- 363
- 369
- 375
