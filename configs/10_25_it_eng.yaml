l1_dataset_path: data/tokenized/10_25_it_eng/l1_train
l2_dataset_path: data/tokenized/10_25_it_eng/l2_train
output_dir: output/10_25_it_eng
tokenizer_path: tokenizer/10_25_it_eng
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 8
- 16
- 32
- 64
- 128
- 165
- 256
- 329
- 493
- 512
- 658
- 822
- 986
- 1150
- 1315
- 1479
- 1643
- 1807
- 1972
- 2136
- 2300
- 2464
- 2629
- 2793
- 2957
- 3122
- 3123
- 3124
- 3126
- 3130
- 3138
- 3154
- 3186
- 3250
- 3378
- 3557
- 3634
- 3992
- 4427
- 4862
- 5297
- 5732
- 6167
- 6602
- 7037
- 7472
- 7907
- 8342
- 8777
- 9212
- 9647
- 10082
- 10517
- 10952
- 11387
- 11822
- 12257
- 12692
- 13127
- 13562
- 13997
- 14432
- 14867
- 15302
- 15737
- 16172
- 16607
- 17042
- 17477
- 17912
- 18347
- 18782
- 19217
- 19652
- 20087
- 20522
- 20957
- 21392
- 21827
- 22262
- 22697
- 23132
- 23567
- 24002
- 24437
