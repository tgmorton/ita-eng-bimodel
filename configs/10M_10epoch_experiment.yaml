# Base configuration for a 10M parameter model training run
output_dir: "models/10M_10epoch"
train_dataset_path: "data/tokenized/10M/train"

# --- Model ---
tokenizer_path: "tokenizer/10M"
model_arch_type: "gpt2"
train_from_scratch: true
model_size_tag: "gpt2-100m"

# --- Training Hyperparameters ---
num_train_epochs: 10
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 5e-4
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: "cosine"
num_warmup_steps: 100

# --- Logging & Saving ---
logging_steps: 5
save_steps: 3

# --- Hardware & Seed ---
use_amp: true
num_workers: 4
seed: 42

checkpoint_schedule:
    - 1
    - 2
    - 4
    - 8
    - 16
    - 32
    - 64
    - 128
    - 256
    - 384
    - 512
    - 768
    - 1024
    - 1365
    - 1707
    - 2048
    - 2316
    - 2584
    - 2853
    - 3121
    - 3122
    - 3123
    - 3125
    - 3129
    - 3137
    - 3153
    - 3185
    - 3249
    - 3377
    - 3505
    - 3633
    - 3889
    - 4145
    - 4486
    - 4828
    - 5169
    - 5437
    - 5705
    - 5974
    - 6242
    - 6243
    - 6244
    - 6246
    - 6250
    - 6258
    - 6274
    - 6306
    - 6370
    - 6498
    - 6626
    - 6754
    - 7010
    - 7266
    - 7607
    - 7949
    - 8290
    - 8558
    - 8826
    - 9095
    - 9363
    - 9364
    - 9365
    - 9367
    - 9371
    - 9379
    - 9395
    - 9427
    - 9491
    - 9619
    - 9747
    - 9875
    - 10131
    - 10387
    - 10728
    - 11070
    - 11411
    - 11679
    - 11947
    - 12216
    - 12484
    - 12485
    - 12486
    - 12488
    - 12492
    - 12500
    - 12516
    - 12548
    - 12612
    - 12740
    - 12868
    - 12996
    - 13252
    - 13508
    - 13849
    - 14191
    - 14532
    - 14800
    - 15068
    - 15337
    - 15605
    - 15606
    - 15607
    - 15609
    - 15613
    - 15621
    - 15637
    - 15669
    - 15733
    - 15861
    - 15989
    - 16117
    - 16373
    - 16629
    - 16970
    - 17312
    - 17653
    - 17921
    - 18189
    - 18458
    - 18726
    - 18727
    - 18728
    - 18730
    - 18734
    - 18742
    - 18758
    - 18790
    - 18854
    - 18982
    - 19110
    - 19238
    - 19494
    - 19750
    - 20091
    - 20433
    - 20774
    - 21042
    - 21310
    - 21579
    - 21847
    - 21848
    - 21849
    - 21851
    - 21855
    - 21863
    - 21879
    - 21911
    - 21975
    - 22103
    - 22231
    - 22359
    - 22615
    - 22871
    - 23212
    - 23554
    - 23895
    - 24163
    - 24431
    - 24700
    - 24968
    - 24969
    - 24970
    - 24972
    - 24976
    - 24984
    - 25000
    - 25032
    - 25096
    - 25224
    - 25352
    - 25480
    - 25736
    - 25992
    - 26333
    - 26675
    - 27016
    - 27284
    - 27552
    - 27821
    - 28089
    - 28090
    - 28091
    - 28093
    - 28097
    - 28105
    - 28121
    - 28153
    - 28217
    - 28345
    - 28473
    - 28601
    - 28857
    - 29113
    - 29454
    - 29796
    - 30137
    - 30405
    - 30673
    - 30942
    - 31210