l1_dataset_path: data/tokenized/50_25_it_eng/l1_train
l2_dataset_path: data/tokenized/50_25_it_eng/l2_train
output_dir: output/bilingual_sweep/50_25_it_eng
tokenizer_path: tokenizer/50_25_it_eng
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 5
- 8
- 10
- 15
- 16
- 20
- 24
- 29
- 32
- 34
- 39
- 44
- 48
- 53
- 58
- 63
- 64
- 67
- 72
- 77
- 82
- 87
- 91
- 96
- 101
- 106
- 110
- 115
- 120
- 125
- 128
- 130
- 134
- 139
- 144
- 149
- 153
- 158
- 163
- 168
- 173
- 177
- 182
- 187
- 192
- 196
- 201
- 206
- 211
- 216
- 220
- 225
- 230
- 235
- 239
- 244
- 249
- 254
- 259
- 263
- 268
- 273
- 278
- 282
- 287
- 292
- 297
- 302
- 306
- 311
- 316
- 321
- 325
- 330
- 335
- 340
- 345
- 349
- 354
- 359
- 364
- 368
- 373
- 378
- 383
- 388
- 392
- 397
- 402
- 407
- 411
- 416
- 421
- 426
- 431
- 435
- 440
- 445
- 450
- 454
- 459
- 464
- 469
- 474
- 475
- 476
- 478
- 480
- 482
- 486
- 490
- 492
- 498
- 503
- 506
- 509
- 515
- 521
- 526
- 532
- 538
- 544
- 549
- 555
- 561
- 567
- 572
- 578
- 584
- 590
- 595
- 601
- 602
- 607
- 613
- 618
- 624
- 630
- 636
- 641
- 647
- 653
- 659
- 664
- 670
- 676
- 682
- 687
- 693
- 699
- 705
- 710
- 716
- 722
- 728
- 733
- 739
- 745
- 751
- 757
