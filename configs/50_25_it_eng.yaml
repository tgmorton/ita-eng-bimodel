l1_dataset_path: data/tokenized/50_25_it_eng/l1_train
l2_dataset_path: data/tokenized/50_25_it_eng/l2_train
output_dir: output/50_25_it_eng
tokenizer_path: tokenizer/50_25_it_eng
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 8
- 16
- 32
- 64
- 128
- 130
- 256
- 260
- 389
- 512
- 519
- 648
- 778
- 907
- 1037
- 1166
- 1296
- 1425
- 1555
- 1684
- 1814
- 1943
- 2073
- 2202
- 2332
- 2461
- 2591
- 2721
- 2850
- 2980
- 3109
- 3239
- 3368
- 3498
- 3627
- 3757
- 3886
- 4016
- 4145
- 4275
- 4404
- 4534
- 4663
- 4793
- 4922
- 5052
- 5182
- 5311
- 5441
- 5570
- 5700
- 5829
- 5959
- 6088
- 6218
- 6347
- 6477
- 6606
- 6736
- 6865
- 6995
- 7124
- 7254
- 7383
- 7513
- 7642
- 7772
- 7902
- 8031
- 8161
- 8290
- 8420
- 8549
- 8679
- 8808
- 8938
- 9067
- 9197
- 9326
- 9456
- 9585
- 9715
- 9844
- 9974
- 10103
- 10233
- 10363
- 10492
- 10622
- 10751
- 10881
- 11010
- 11140
- 11269
- 11399
- 11528
- 11658
- 11787
- 11917
- 12046
- 12176
- 12305
- 12435
- 12564
- 12694
- 12824
- 12825
- 12826
- 12828
- 12832
- 12840
- 12856
- 12888
- 12952
- 13080
- 13259
- 13336
- 13694
- 14129
- 14564
- 14999
- 15434
- 15869
- 16304
- 16739
- 17174
- 17609
- 18044
- 18479
- 18914
- 19349
- 19784
- 20219
- 20654
- 21089
- 21524
- 21959
- 22394
- 22829
- 23264
- 23699
- 24134
- 24569
- 25004
- 25439
- 25874
- 26309
- 26744
- 27179
- 27614
- 28049
- 28484
- 28919
- 29354
- 29789
- 30224
- 30659
- 31094
- 31529
- 31964
- 32399
- 32834
- 33269
- 33704
- 34139
