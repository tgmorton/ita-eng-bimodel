# Base configuration for a 10M parameter model training run
output_dir: "models/25M"
train_dataset_path: "data/tokenized/25M/train"

# --- Model ---
tokenizer_path: "tokenizer/25M"
model_arch_type: "gpt2"
train_from_scratch: true
model_size_tag: "gpt2-100m"

# --- Training Hyperparameters ---
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 5e-4
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: "cosine"
num_warmup_steps: 100

# --- Logging & Saving ---
logging_steps: 5
save_steps: 3

# --- Hardware & Seed ---
use_amp: true
num_workers: 4
seed: 42

checkpoint_schedule:
- 1
- 2
- 4
- 8
- 16
- 32
- 64
- 128
- 192
- 256
- 384
- 512
- 598
- 683
- 853
- 1024
- 1126
- 1229
- 1332
- 1434
- 1536
- 1638
- 1740
- 1843
- 1946
- 2048
- 2156
- 2264
- 2372
- 2479
- 2587
- 2695
- 2802
- 2910
- 3018
- 3126
- 3385
- 3644
- 3903
- 4162
- 4421
- 4680
- 4939
- 5198
- 5457
- 5716
- 5975
- 6234
- 6493
- 6752