l1_dataset_path: data/tokenized/10_25_eng_it/l1_train
l2_dataset_path: data/tokenized/10_25_eng_it/l2_train
output_dir: output/bilingual_sweep/10_25_eng_it
tokenizer_path: tokenizer/10_25_eng_it
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 6
- 8
- 12
- 16
- 18
- 24
- 30
- 32
- 36
- 42
- 48
- 54
- 59
- 65
- 71
- 77
- 83
- 89
- 95
- 101
- 107
- 113
- 114
- 115
- 117
- 118
- 121
- 123
- 128
- 129
- 133
- 138
- 142
- 145
- 147
- 152
- 157
- 162
- 166
- 171
- 176
- 177
- 181
- 186
- 191
- 195
- 200
- 205
- 210
- 215
- 219
- 224
- 229
- 234
- 239
- 244
- 248
- 253
- 258
- 263
- 268
- 272
- 277
- 282
- 287
- 292
- 297
- 301
- 306
- 311
- 316
- 321
- 325
- 330
- 335
- 340
- 345
- 350
