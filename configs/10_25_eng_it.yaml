l1_dataset_path: data/tokenized/10_25_eng_it/l1_train
l2_dataset_path: data/tokenized/10_25_eng_it/l2_train
output_dir: output/10_25_eng_it
tokenizer_path: tokenizer/10_25_eng_it
architectures_path: configs/model_architectures.yaml
train_from_scratch: true
model_arch_type: gpt2
model_size_tag: gpt2-100m
num_train_epochs: 1
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
learning_rate: 0.0005
weight_decay: 0.01
max_grad_norm: 1.0
lr_scheduler_type: linear
num_warmup_steps: 100
use_amp: true
num_workers: 4
seed: 42
logging_steps: 100
save_steps: 500
checkpoint_schedule:
- 1
- 2
- 4
- 8
- 16
- 32
- 64
- 128
- 256
- 446
- 512
- 892
- 1338
- 1783
- 2229
- 2675
- 3121
- 3566
- 4012
- 4458
- 4904
- 5349
- 5795
- 6241
- 6687
- 7132
- 7578
- 8024
- 8470
- 8471
- 8472
- 8474
- 8478
- 8486
- 8502
- 8534
- 8598
- 8608
- 8726
- 8746
- 8884
- 8982
- 9021
- 9159
- 9297
- 9434
- 9572
- 9710
- 9847
- 9985
- 10123
- 10261
- 10398
- 10536
- 10674
- 10811
- 10949
- 11087
- 11224
- 11362
- 11500
- 11637
- 11775
- 11913
- 12051
- 12188
- 12326
- 12464
- 12601
- 12739
- 12877
- 13014
- 13152
- 13290
- 13427
- 13565
- 13703
- 13841
- 13978
- 14116
- 14254
- 14391
- 14529
- 14667
- 14804
- 14942
- 15080
- 15218
